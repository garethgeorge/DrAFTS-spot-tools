const {ArgumentParser} = require("argparse");
const {Spinner} = require("cli-spinner");
const cliprogress = require("cli-progress");
const util = require("util");
const getRandomInt = util.promisify(require("secure_random").getRandomInt);
const assert = require("assert");
const process = require("process");
const zlib = require("zlib");
const path = require("path");

const {db, format} = require("../src/lib/db");
const {EC2} = require("../src/lib/aws");
const Fetcher = require("../src/helpers/fetch/Fetcher");
const {addToTime} = require("../src/helpers/common/timeutils");

const config = require("../src/lib/config");
const {getPGraphAdv} = require("../src/helpers/predict/predict");
const debug = require("debug")("script");

const {streamLineByLine} = require("../src/helpers/common/fsutil");
const asyncMap = require("../src/lib/async-map");

const fs = require("async-file");

const parser = new ArgumentParser({
  version: "1.0.0",
  addHelp: true,
  description: "used to fetch or update the dataset from AWS's servers!"
});

parser.addArgument(
  ["--region"],
  {
    help: "filter on the region",
  }
)

parser.addArgument(
  ["--az"],
  {
    help: "filter on the az"
  }
)

parser.addArgument(
  ["--insttype"],
  {
    help: "filter on the instance type",
  }
)

parser.addArgument(
  ["intervalsFile"],
  {
    help: "a CSV file formatted as [region, az, inst_type, start, stop] of intervals of continuous date" + 
    " this is the file generated by results-analysis/identify-intervals.py, a python script which" + 
    " runs on the output of bin/status.js when it is in isTTY mode to identify intervals over which." + 
    " data is available",
  }
)

parser.addArgument(
  ["--backlogDays"],
  {
    defaultValue: 30,
    type: "int",
    // required: true
  }
)

parser.addArgument(
  ["--samples"],
  {
    required: true,
    type: "int",
    defaultValue: 100,
    help: "the number of 'experiments' to run"
  }
)

parser.addArgument(
  ["--duration"],
  {
    required: true,
    type: "int"
  }
)

parser.addArgument(
  ["--binpath"],
  {
    help: "the path to the bmbp_ts and pred-distribution binaries",
    defaultValue: config.BinPath
  }
);

parser.addArgument(
  "--quant",
  {
    defaultValue: 0.975,
    type: "float",
    help: "the quantile parameter to bmbp_ts"
  }
)

parser.addArgument(
  "--conf",
  {
    defaultValue: 0.01,
    type: "float",
    help: "the confidence parameter to bmbp_ts"
  }
)

parser.addArgument(
  ["--outputFile"],
  {
    help: "the path to the JSON file where we should dump our output data, " + 
    "if an interval file was provided then this will contain the recommended " +
    "bid price for each interval. Otherwise it will contain a pgraph file " + 
    "mapping bid prices to durations.",
    defaultValue: "output.json",
  }
)

parser.addArgument(
  ["--includePGraph"],
  {
    help: "should we store the pgrap in the results object",
    action: "storeTrue",
  }
)


parser.addArgument(
  ["--logFile"],
  {
    help: "file where we will store redirected output",
  }
)

const args = parser.parseArgs();
const msOneHour = 3600 * 1000;
const msOneDay = (24 * msOneHour);

/*
  given a pgraph file and the required duration, this function returns
  a recommended bid price to issue for the duration
*/
const calculateBidPrice = (pgraph, duration) => {
  assert(pgraph, "pgraph must not be null or undefined");
  assert(pgraph.length > 0, "pgraph is expected to have at least 1 entry.");
  for (const row of pgraph) {
    if (row.duration >= duration) {
      return row.price;
    }
  }
  return pgraph[pgraph.length - 1].price;
}

/*
  run an experiment with a given interval
*/
const runExperiment = async (interval) => {
  const {
    region,
    az, 
    insttype, 
  } = interval;

  const ec2 = new EC2(region, config.AWS.AccessKeyId, config.AWS.SecretAccessKey);
  const fetcher = new Fetcher(db, ec2, args.az);

  // compute a few time intervals
  const sampleInterval = {
    start: addToTime(interval.start, args.backlogDays),
    stop: interval.stop 
  };

  const backlogInterval = {
    start: interval.start,
    stop: sampleInterval.start
  };

  console.log(`running experiment in region: ${region} az: ${az} insttype: ${insttype}`);
  console.log("\tbacklog interval: ", backlogInterval.start, " - ", backlogInterval.stop)
  console.log("\tsample interval: ", sampleInterval.start, " - ", sampleInterval.stop)
  console.log(`\t\ttotal duration of backlog: ${(backlogInterval.stop.getTime() - backlogInterval.start.getTime()) / msOneDay}`);
  console.log(`\t\ttotal duration for samples (days): ${(sampleInterval.stop.getTime() - sampleInterval.start.getTime()) / msOneDay}`);
  console.log(`\t\tnumber of samples in backlog: ${(await fetcher.debugGetHistoryRecordsInInterval(insttype, backlogInterval.start, backlogInterval.stop))}`)
  console.log(`\t\tnumber of samples in sample interval: ${(await fetcher.debugGetHistoryRecordsInInterval(insttype, sampleInterval.start, sampleInterval.stop))}`)


  if (sampleInterval.stop.getTime() - sampleInterval.start.getTime() < args.duration * msOneHour) {
    console.log("\t\tERROR: sample interval is empty! writing out an error");
    return {
      status: "error",
      error: "sample time interval was empty, no data to work with"
    };
  }
  
  console.log("generating sample times");
  let secondsInSampleRange = 
    (sampleInterval.stop.getTime() - sampleInterval.start.getTime()) / 1000;
  secondsInSampleRange -= args.duration * 3600;
  const sampleTimes = [];
  for (let i = 0; i < args.samples; ++i) {
    const offset = (await getRandomInt(0, secondsInSampleRange));
    sampleTimes.push(addToTime(sampleInterval.start, 0, 0, 0, offset));
  }
    
  sampleTimes.sort((date1, date2) => {
    if (date1.getTime() > date2.getTime()) return 1;
    if (date1.getTime() < date2.getTime()) return -1;
    return 0;
  }); // from https://gist.github.com/onpubcom/1772996

  console.log(`\tgenerated ${sampleTimes.length} times to sample at`);
  
  console.log("computing and pre-caching the pgraph objects for the time range " + 
    "of the experiment.");
  const progressBar = new cliprogress.Bar({}, cliprogress.Presets.shades_classic);
  progressBar.start(sampleTimes.length, 0);
  let completedCount = 0;
  
  const pgraphArray = await getPGraphAdv(db, {
    workdir: config.Workdir,
    region: args.region,
    az: args.az,
    insttype: args.insttype,
    daterange: {
      start: backlogInterval.start, 
      end: sampleInterval.stop,
    },
    binpath: args.binpath,
    bmbp_ts_args: {
      quant: args.quant,
      conf: args.conf,
    },
    onResult: (result) => {
      completedCount++;
      progressBar.update(completedCount);
    },
  }, sampleTimes);
  progressBar.stop();

  console.log("pgraph array contains: " + pgraphArray.length + " pgraphs, we passed " + sampleTimes.length + " times. ");
  console.log(`\t${sampleTimes.length - pgraphArray.length} of those were empty intervals`);

  pgraphArray.sort((pgraph1, pgraph2) => {
    if (pgraph1.interval.stop > pgraph2.interval.stop) return 1;
    if (pgraph1.interval.stop < pgraph2.interval.stop) return -1;
    return 0;
  });

  const pgraphForTime = (time) => {
    const time_ms = time.getTime();
    for (let i = pgraphArray.length - 1; i >= 0; --i) {
      if (time_ms >= pgraphArray[i].interval.stop.getTime()) {
        debug("pgraph found at index %o", i);
        return pgraphArray[i];
      }
    }
    return pgraphArray[0];
  }


  // assert(pgraphArray.length === sampleTimes.length, "expected pgraphArray.length (" + pgraphArray.length + ") to equal times.length (" + sampleTimes.length + ")");

  console.log("running the simulation, and computing pre-mature terminations");
  const results = {}
  results.interval = interval 
  results.pgraphs = args.includePGraph ? pgraphArray : null;
  results.terminations = 0;
  results.samples = [];
  const log = [];
  for (let i = 0; i < sampleTimes.length; ++i) {
    const time = sampleTimes[i];
    const timeEnd = addToTime(time, 0, args.duration);
    
    // first we get all spot prices that might be inside the time range,
    const historyObjects = await fetcher.fetchTimeRangeFromDB(time, timeEnd, insttype);
    // then we add in the spot price just before the time range started to
    // make sure we have every price that covers the time interval
    historyObjects.push(await fetcher.getSpotPriceForTime(time, insttype));
    const spotPrices = historyObjects.map(x => x.SpotPrice);
    spotPrices.sort();
    
    // get the pgraph and compute a bid price
    debug("sample #%o", i);
    
    const pgraphResult = pgraphForTime(time);
    const correspondingPGraph = pgraphResult.pgraph;
    debug("\trows in pgraph %o", correspondingPGraph.length);
    const bidprice = calculateBidPrice(correspondingPGraph, args.duration);
    
    debug("\tspot prices at time %o were %o", time, spotPrices);
    debug("\trecommended bid price is %o", bidprice);

    const terminated = spotPrices[spotPrices.length - 1] > bidprice;
    if (terminated) {
      debug("\tterminated due to spot price %o exceeding bid price of %o", 
        spotPrices[spotPrices.length - 1], bidprice);
    }
    debug("\tterminated: %o", terminated);

    if (terminated)
      results.terminations++;
    
    // assert(sampleTimes[i].getTime() === pgraphResult.interval.stop.getTime(), 
    //   "expected times[i].getTime() === pgraphResult.interval.start.getTime(), " + 
    //   "but they mismatched " + sampleTimes[i] + " vs " + pgraphResult.interval.end);
    
    results.samples.push({
      time: sampleTimes[i],
      duration: args.duration,
      bidprice: bidprice,
      prices: spotPrices,
      terminated: terminated
    });
  }

  console.log("terminations: " + results.terminations);

  return results;
}

/*
  ENTRY POINT
*/

(async () => {
  let result = null;

  debug("loading intervals from args.intervalsFile: " + args.intervalsFile);

  const intervals = []

  for await (const line of streamLineByLine(fs.createReadStream(args.intervalsFile))) {
    const arr = line.split(",");
    if (arr.length !== 5) continue ;
    const region = arr[0].trim();
    const az = arr[1].trim();
    const insttype = arr[2].trim();
    const start = new Date(arr[3]);
    const stop = new Date(arr[4]);

    if ((!args.region || region === args.region) && 
        (!args.az || az === args.az) && 
        (!args.insttype || args.insttype === insttype)) {
      // is it a long enough for us to run our experiments
      if ((stop.getTime() - start.getTime()) / (1000 * 3600) > args.backlogDays * 24 + args.duration) {
        intervals.push({
          region, az, insttype, start, stop
        });
      } else {
        console.log("skipping interval ", start, " - ", stop, " because the duration is less than ", args.backlogDays);
      }
    } 
  }

  console.log(`LOADED ${intervals.length} intervals to test`);

  result = await asyncMap(intervals, async (interval) => {
    console.log("running experiments on interval ", interval.start, " - ", interval.stop);
    try {
      return await runExperiment(interval);
    } catch (e) {
      console.log(e);
      return {
        error: e.toString()
      }
    }
  });

  console.log("writing results to '" + args.outputFile + "'");
  
  let output = fs.createWriteStream(args.outputFile);
  if (path.extname(args.outputFile) === ".gz") {
    debug("the output file is a .gz file, we will create a compressed output stream");
    const zipper = zlib.createGzip();
    zipper.pipe(output);
    output = zipper;
  }

  output.write(JSON.stringify({
    results: result,
    args: args
  }, null, 2));

  output.end();
    
  db.end();

  console.log("done.");
  
})()
